{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5556e9",
   "metadata": {},
   "source": [
    "# Demonstration and error analysis\n",
    "In this notebook I demonstrate how the models I trained can be used to make predictions on novel hypothesis - premise pairs. Furthes, I present my evaluation results and I carry out an error analysis, both for the NLI task and for the sentence embeddings.\n",
    "\n",
    "## Dependencies\n",
    "* [CheckList](https://github.com/marcotcr/checklist)\n",
    "* Torch\n",
    "* Pandas\n",
    "* Numpy\n",
    "* Json\n",
    "* Copy\n",
    "\n",
    "Further, the model checkpoint should be downloaded from [this link](https://drive.google.com/drive/folders/18EWKTYv4CsF8mxgE7K4Ym6zHtqR6w6fF?usp=sharing) and placed in the `model_checkpoints` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import models\n",
    "import torch \n",
    "import config\n",
    "import mutils\n",
    "\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = config.seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49c283",
   "metadata": {},
   "source": [
    "## Performance scores\n",
    "Here I will present the performance scores of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae880f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../eval_results/final_model_results.json') as json_file:\n",
    "        nli_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa06735",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame.from_dict(nli_dict, orient='index')\n",
    "results_table = results_table[['dev_acc', 'test_acc', 'micro', 'macro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d3eb7",
   "metadata": {},
   "source": [
    "Performance scores similar to Table 3 in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d714264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>micro</th>\n",
       "      <th>macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>65.57</td>\n",
       "      <td>65.25</td>\n",
       "      <td>79.16</td>\n",
       "      <td>77.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>79.45</td>\n",
       "      <td>78.81</td>\n",
       "      <td>76.98</td>\n",
       "      <td>76.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstm</th>\n",
       "      <td>78.51</td>\n",
       "      <td>78.55</td>\n",
       "      <td>79.63</td>\n",
       "      <td>78.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstmpool</th>\n",
       "      <td>82.98</td>\n",
       "      <td>82.79</td>\n",
       "      <td>81.37</td>\n",
       "      <td>80.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dev_acc  test_acc  micro  macro\n",
       "base          65.57     65.25  79.16  77.75\n",
       "lstm          79.45     78.81  76.98  76.25\n",
       "bilstm        78.51     78.55  79.63  78.88\n",
       "bilstmpool    82.98     82.79  81.37  80.50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95faf5",
   "metadata": {},
   "source": [
    "The performance scores for the LSTM and the BiLSTM with max pooling correspond to the results of Conneau et al. (2017) within a 3% margin for the NLI task and a 6% margin for SentEval. Also corresponding to Conneau et al. the BiLSTM max pooling model performs the best, both for NLI and for SentEval. Interestingly however, the base model outperforms the LSTM for SentEval, and performs equal to BiLSTM on these tasks, while its performance is lower for the NLI task.\n",
    "\n",
    "Performance scores for the individual SentEval tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c5b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MR</th>\n",
       "      <th>CR</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>MPQA</th>\n",
       "      <th>SST2</th>\n",
       "      <th>TREC</th>\n",
       "      <th>SICKEntailment</th>\n",
       "      <th>MRPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>74.88</td>\n",
       "      <td>78.46</td>\n",
       "      <td>90.11</td>\n",
       "      <td>84.85</td>\n",
       "      <td>78.21</td>\n",
       "      <td>67.31</td>\n",
       "      <td>80.8</td>\n",
       "      <td>71.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>72.13</td>\n",
       "      <td>77.73</td>\n",
       "      <td>85.96</td>\n",
       "      <td>84.84</td>\n",
       "      <td>76.26</td>\n",
       "      <td>61.45</td>\n",
       "      <td>82.2</td>\n",
       "      <td>73.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstm</th>\n",
       "      <td>72.40</td>\n",
       "      <td>79.26</td>\n",
       "      <td>89.52</td>\n",
       "      <td>85.06</td>\n",
       "      <td>78.67</td>\n",
       "      <td>73.06</td>\n",
       "      <td>83.4</td>\n",
       "      <td>72.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstmpool</th>\n",
       "      <td>75.76</td>\n",
       "      <td>81.78</td>\n",
       "      <td>91.57</td>\n",
       "      <td>85.64</td>\n",
       "      <td>79.93</td>\n",
       "      <td>75.26</td>\n",
       "      <td>84.4</td>\n",
       "      <td>74.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MR     CR   SUBJ   MPQA   SST2   TREC  SICKEntailment   MRPC\n",
       "base        74.88  78.46  90.11  84.85  78.21  67.31            80.8  71.20\n",
       "lstm        72.13  77.73  85.96  84.84  76.26  61.45            82.2  73.11\n",
       "bilstm      72.40  79.26  89.52  85.06  78.67  73.06            83.4  72.55\n",
       "bilstmpool  75.76  81.78  91.57  85.64  79.93  75.26            84.4  74.31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('eval_results/final_task_results.json') as json_file:\n",
    "        sent_eval_scores = json.load(json_file)\n",
    "        \n",
    "sent_eval_results = pd.DataFrame.from_dict(sent_eval_scores, orient='index')\n",
    "sent_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5735130",
   "metadata": {},
   "source": [
    "The base model performs particularly well on SUBJ (subjectivity status) and MPQA (opinion polarity). All models perform relatively low on TREC (question-type classification), particulary the LSTM achieves a low performance on this task. All models achieve satisfying results on the SUBJ task.\n",
    "\n",
    "BiLSTMPool performs the best on all tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e934f",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "Here I will demonstrate how to make predictions for a new hypothesis-premise pair with one of the models.\n",
    "### Load models\n",
    "load all trained models + the GloVe embeddings of the tokens in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd742b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load bilstm encoder\n",
      "load bilstm maxpool encoder\n"
     ]
    }
   ],
   "source": [
    "base_nli, _ = mutils.load_model('base', '../model_checkpoints/base_model_final', None)\n",
    "lstm_nli, lstm_lstm = mutils.load_model('lstm', '../model_checkpoints/lstm_nli_model_final', '../model_checkpoints/lstm_lstm_model_final')\n",
    "bilstm_nli, bilstm_lstm = mutils.load_model('bilstm', '../model_checkpoints/bilstm_nli_model_final', '../model_checkpoints/bilstm_lstm_model_final')\n",
    "bilstmpool_nli, bilstmpool_lstm = mutils.load_model('bilstmpool', '../model_checkpoints/bilstmpool_nli_model_final', '../model_checkpoints/bilstmpool_lstm_model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66d5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_word_dict.json') as json_file:\n",
    "    word_dict = json.load(json_file)\n",
    "\n",
    "embedding_model = mutils.load_embeddings(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac3423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_translate = {0 : 'entailment', 1: 'neutral', 2: 'contradiction'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model name\n",
    "model = 'bilstm'\n",
    "# select corresponding nli model\n",
    "nli = bilstm_nli\n",
    "#define premise (s1) and hypothesis (s2)\n",
    "s1 = 'the dog was hungry'\n",
    "s2 = 'the cat was hungry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cad1ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for bilstm model for \n",
      "Premise: the dog was hungry \n",
      "Hypothesis: the cat was hungry\n",
      "\n",
      "Prediction = 2 : contradiction\n"
     ]
    }
   ],
   "source": [
    "#make prediciton\n",
    "prediction = mutils.predict(model, embedding_model, nli, s1, s2)\n",
    "print(f\"Prediction for {model} model for \\nPremise: {s1} \\nHypothesis: {s2}\\n\") \n",
    "print(f\"Prediction = {prediction} : {prediction_translate[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a6903",
   "metadata": {},
   "source": [
    "Here, the bilstm model makes a false prediction, as I would consider the premise and the hypothesis to be neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce8bcc",
   "metadata": {},
   "source": [
    "## Error analysis for NLI\n",
    "\n",
    "In this Section, I will try out various difficult NLI testcases. In order to investigate whether the models make structural mistakes, I create 100 example sentences per testcase using [CheckList](https://github.com/marcotcr/checklist), and report on the performance of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c20cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 0, 1:0, 2:0}\n",
    "\n",
    "#create a dict that keeps track of predictions per model\n",
    "results_dict = {'base': copy.deepcopy(label_dict),\n",
    "               'lstm' : copy.deepcopy(label_dict),\n",
    "               'bilstm': copy.deepcopy(label_dict),\n",
    "               'bilstmpool': copy.deepcopy(label_dict)}\n",
    "\n",
    "model_tuples = [('base', base_nli),('lstm', lstm_nli), ('bilstm', bilstm_nli), ('bilstmpool', bilstmpool_nli)]\n",
    "\n",
    "def predict_all_models(result_dict, s1, s2):\n",
    "    \"\"\"\n",
    "    Makes predictions on a premise-hypothesis pair for all four models, stores predictions in results dict\n",
    "    \"\"\"\n",
    "    for model, nli in model_tuples:\n",
    "        prediction = mutils.predict(model, embedding_model, nli, s1, s2)\n",
    "        result_dict[model][prediction] += 1\n",
    "    return result_dict\n",
    "\n",
    "def print_performances(result_dict, nsamples, correct_label):\n",
    "    \"\"\"\n",
    "    Prints out a table with the performance scores on the testcase per model\n",
    "    \"\"\"\n",
    "    performances = {}\n",
    "    for model, _ in model_tuples:\n",
    "        model_scores = copy.deepcopy(result_dict[model])\n",
    "        model_scores['correct'] = str(round(model_scores[correct_label] / nsamples * 100,1)) + '%'\n",
    "        performances[model] = model_scores\n",
    "    performance_table = pd.DataFrame.from_dict(performances, orient='index')\n",
    "    print(performance_table)\n",
    "\n",
    "def eval_test(eval_sents, nsamples, correct_label):\n",
    "    \"\"\"\n",
    "    Evaluates all models on a testcase of {nsamples} evaluation sentences;\n",
    "    prints out a table with per-model performance scores\n",
    "    \"\"\"\n",
    "    result_dict = copy.deepcopy(results_dict)\n",
    "    for n, sent in enumerate(eval_sents):\n",
    "        try:\n",
    "            s1, s2 = sent.split(';')\n",
    "            \n",
    "        except:\n",
    "            nsamples = nsamples - 1\n",
    "            continue\n",
    "        if n == 0:\n",
    "            print(f\"{nsamples} samples of the following structure where the correct label is \" +\n",
    "                  f\"{correct_label} ({prediction_translate[correct_label]}):\")\n",
    "            print(f\"Premise: '{s1}' \\nHypothesis '{s2}'\")\n",
    "        result_dict = predict_all_models(result_dict, s1, s2)\n",
    "    print_performances(result_dict, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116db77",
   "metadata": {},
   "source": [
    "### Negation\n",
    "In this testcase, I will investigate whether the models can recognize a simple negation to be a contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f34bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a6f8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "negation = editor.template('{first_name} is from {country}; {first_name} is not from {country} ')\n",
    "\n",
    "np.random.seed(seed) \n",
    "negation_sents = np.random.choice(negation.data, nsamples)\n",
    "correct_label = 2 #contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73c13a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 2 (contradiction):\n",
      "Premise: 'Scott is from Poland' \n",
      "Hypothesis ' Scott is not from Poland '\n",
      "             0   1   2 correct\n",
      "base        43  56   1    1.0%\n",
      "lstm         1   0  99   99.0%\n",
      "bilstm       1   0  99   99.0%\n",
      "bilstmpool   3   0  97   97.0%\n"
     ]
    }
   ],
   "source": [
    "eval_test(negation_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f54639",
   "metadata": {},
   "source": [
    "The base model is unable to handle simple negation: it predicts either entailment or neutral - the mean of the token embeddings does not appear to capture contradictions. The LSTM-based models do handle these cases correctly\n",
    "\n",
    "## Negetion in one part of the sentence, entailment in the relevant part\n",
    "In the following sentences, these is a negation in the first clause of the premise but *not* in the second clause, and the hypothesis entails the second clause. A model that can recognizes negation could struggle here, if it overestimates the scope of the negation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5434d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0 (entailment):\n",
      "Premise: 'Alan does not live in Anaheim but they are from Bulgaria' \n",
      "Hypothesis ' Alan is from Bulgaria '\n",
      "             0   1   2 correct\n",
      "base         1  56  43    1.0%\n",
      "lstm        85   0  15   85.0%\n",
      "bilstm      20   7  73   20.0%\n",
      "bilstmpool  97   0   3   97.0%\n"
     ]
    }
   ],
   "source": [
    "negation_entailment = editor.template('{first_name} does not live in {city} but they are from {country}; {first_name} is from {country} ')\n",
    "np.random.seed(seed) \n",
    "negation_entailment_sents = np.random.choice(negation_entailment.data, nsamples)\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(negation_entailment_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e4a45",
   "metadata": {},
   "source": [
    "As in the previous task, the base model cannot do this task correctly: interestingly however, here the model predicts contradiction/neutral, so it does appear to recognize the negation here (or, it does not recognize the similarity between the premise and the hypothesis). \n",
    "\n",
    "Even more interestingly, the BiLSTM model performs very poorly on this task -consistently predicting contradiction-, while the other LSTM-based models do perform well.\n",
    "\n",
    "\n",
    "\n",
    "## Agent - patient disambiguation in active/passive sentences\n",
    "In the following two tests, I investigate whether the models can correctly distinguish between agents and patients between active and passive constructions. In the first test the patient is the object in the active premise, while it is the subject in the passive hypothesis: here the models should predict entailment. \n",
    "\n",
    "In the second test, the agent is the subject in the active premise - but this same person is also the subject in the passive hypothesis, making it the patient. Here, the models should thus predict neutral or contradition, but *not* entailment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae1f315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of verbs to use in the test cases\n",
    "passive_verbs = ['kissed', 'killed', 'hurt', 'touched', 'ignored', 'silenced', 'hit', 'greeted']\n",
    "english_firstname = editor.lexicons.female_from.United_Kingdom + editor.lexicons.male_from.United_Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b571f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0 (entailment):\n",
      "Premise: 'Christopher hit Hannah' \n",
      "Hypothesis ' Hannah was hit by Christopher'\n",
      "              0   1  2 correct\n",
      "base        100   0  0  100.0%\n",
      "lstm        100   0  0  100.0%\n",
      "bilstm      100   0  0  100.0%\n",
      "bilstmpool   71  21  8   71.0%\n"
     ]
    }
   ],
   "source": [
    "active_passive = editor.template('{first_name} {verb} {first}; {first} was {verb} by {first_name}', first=english_firstname, verb=passive_verbs)\n",
    "\n",
    "np.random.seed(seed) \n",
    "active_passive_sents = np.random.choice(active_passive.data, nsamples)\n",
    "\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(active_passive_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "411939d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 1 (neutral):\n",
      "Premise: 'Christopher hit Hannah' \n",
      "Hypothesis ' Christopher was hit by Hannah'\n",
      "              0  1  2 correct\n",
      "base        100  0  0    0.0%\n",
      "lstm        100  0  0    0.0%\n",
      "bilstm      100  0  0    0.0%\n",
      "bilstmpool  100  0  0    0.0%\n"
     ]
    }
   ],
   "source": [
    "active_passive = editor.template('{first_name} {verb} {first}; {first_name} was {verb} by {first}', first=english_firstname, verb=passive_verbs)\n",
    "\n",
    "np.random.seed(seed) \n",
    "active_passive_sents = np.random.choice(active_passive.data, nsamples)\n",
    "\n",
    "correct_label = 1 #neutral\n",
    "\n",
    "eval_test(active_passive_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae892f",
   "metadata": {},
   "source": [
    "This test-pair gives some interesting results. Firstly, while all models perform well on the first test, they all obtain a zero-score on the second test - even though the names of the subject and the object are swapped between the two hypotheses, the models all still make the same prediction. The models thus all fail consistently at agent-patient disambiguation.\n",
    "\n",
    "Secondly, the BiLSTMPool model obtains the lowest score at the first testcase, which is interesting since this is the best model overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd8417",
   "metadata": {},
   "source": [
    "## Long premise - short hypothesis\n",
    "\n",
    "In this test I evaluate whether the models can extract the relevant information from a longer premise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65e7b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = ['hit', 'kicked', 'stopped', 'touched', 'missed', 'smashed']\n",
    "# lists of sentence fillers to increase the distance between the agent and the predicate\n",
    "precedents = ['nearly falling down', 'missing the past three games', 'celebrating a perfect streak', 'suffering from a knee injury', 'appearing so fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef49e243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0 (entailment):\n",
      "Premise: 'Edward, after suffering from a knee injury, finnaly stopped the ball' \n",
      "Hypothesis ' Edward stopped the ball'\n",
      "              0   1   2 correct\n",
      "base         22  49  29   22.0%\n",
      "lstm         98   0   2   98.0%\n",
      "bilstm       96   4   0   96.0%\n",
      "bilstmpool  100   0   0  100.0%\n"
     ]
    }
   ],
   "source": [
    "long_distance = editor.template(\"{first_name}, after {filler}, finnaly {verb} the ball; {first_name} {verb} the ball\", verb=verbs, filler=precedents)\n",
    "\n",
    "np.random.seed(seed) \n",
    "long_d_sents = np.random.choice(long_distance.data, nsamples)\n",
    "\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(long_d_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0567f0",
   "metadata": {},
   "source": [
    "These results show that all results except the base model can perform this test satisfyingly.\n",
    "\n",
    "## Synonyms / hypernym recognition\n",
    "\n",
    "In this final test, I evaluate whether the models recognize synonyms and some hypernyns correctly, and thus predict entailment for the test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ba8eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_words = [('author', 'writer'), ('surgeon', 'doctor'), ('server', 'waiter'), ('chef','cook'), \n",
    "                 ('educator','teacher'), ('professor','academic'), ('person','human'), ('actor','performer'),\n",
    "                 ('musician', 'artist'), ('hairdresser', 'hairstylist')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9679b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0 (entailment):\n",
      "Premise: 'Tim is a person' \n",
      "Hypothesis ' Tim a human'\n",
      "             0   1   2 correct\n",
      "base        18  73   9   18.0%\n",
      "lstm        55  36   9   55.0%\n",
      "bilstm      72  12  16   72.0%\n",
      "bilstmpool  69  18  13   69.0%\n"
     ]
    }
   ],
   "source": [
    "synonyms = editor.template(\"{first_name} is {a:occupation[0]}; {first_name} {a:occupation[1]}\", occupation=synonym_words, filler=precedents)\n",
    "\n",
    "np.random.seed(seed) \n",
    "synonym_sents = np.random.choice(synonyms.data, nsamples)\n",
    "\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(synonym_sents, nsamples, correct_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b686d",
   "metadata": {},
   "source": [
    "This task appears to be rather difficult, none of the models achieve over 72%. The best performance is obtained by the BiLSTM model and the worst performance by the base model. Most mistakes are classifying a neutral relation between the sentence pairs - this could indicate that the relations between the synonyms/hyper(/o)nyms are not satisfyingly represented in the sentence embeddings, causing the model to consider the sentences to be unrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1e1b1",
   "metadata": {},
   "source": [
    "# Sent embeddings evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcce00",
   "metadata": {},
   "source": [
    "Finally, I will investigate a few things:\n",
    "* cosine similarity of sents containing syno/hyper nyms\n",
    "* visualizations of some sentences - token importantce , following Conneau et al\n",
    "* visualize pos - neg sentiment sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06fc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/dataset-sst2', \n",
    "                 nrows=100, sep='\\t', names=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3cdaad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace({0: 'negative', 1: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d96d01c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mutils' has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14149/3018178698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bilstm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'mutils' has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "model = 'bilstm'\n",
    "e = mutils.encode(model, embedding_model, df['text'], device, bistm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0a58b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'config',\n",
       " 'create_batches',\n",
       " 'device',\n",
       " 'get_UVY',\n",
       " 'get_batch',\n",
       " 'get_sent_embedding',\n",
       " 'get_word_embedding',\n",
       " 'load_embeddings',\n",
       " 'load_model',\n",
       " 'load_word_dict',\n",
       " 'models',\n",
       " 'nn',\n",
       " 'np',\n",
       " 'output_dict',\n",
       " 'pad',\n",
       " 'pad_singlebatch',\n",
       " 'predict',\n",
       " 'preprocess_sentence_data',\n",
       " 'torch',\n",
       " 'word_tokenize']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72c9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

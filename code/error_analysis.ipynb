{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import models\n",
    "import torch \n",
    "import config\n",
    "import mutils\n",
    "\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49c283",
   "metadata": {},
   "source": [
    "## Performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae880f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../eval_results/final_model_results.json') as json_file:\n",
    "        nli_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa06735",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame.from_dict(nli_dict, orient='index')\n",
    "results_table = results_table[['dev_acc', 'test_acc', 'micro', 'macro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d3eb7",
   "metadata": {},
   "source": [
    "Performance scores similar to Table 3 in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d714264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>micro</th>\n",
       "      <th>macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>65.57</td>\n",
       "      <td>65.25</td>\n",
       "      <td>79.16</td>\n",
       "      <td>77.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>79.45</td>\n",
       "      <td>78.81</td>\n",
       "      <td>76.98</td>\n",
       "      <td>76.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstm</th>\n",
       "      <td>78.51</td>\n",
       "      <td>78.55</td>\n",
       "      <td>79.63</td>\n",
       "      <td>78.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstmpool</th>\n",
       "      <td>82.98</td>\n",
       "      <td>82.79</td>\n",
       "      <td>81.37</td>\n",
       "      <td>80.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dev_acc  test_acc  micro  macro\n",
       "base          65.57     65.25  79.16  77.75\n",
       "lstm          79.45     78.81  76.98  76.25\n",
       "bilstm        78.51     78.55  79.63  78.88\n",
       "bilstmpool    82.98     82.79  81.37  80.50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95faf5",
   "metadata": {},
   "source": [
    "The performance scores for the LSTM and the BiLSTM with max pooling correspond to the results of Conneau et al. (2017) within a 3% margin for the NLI task and a 6% margin for SentEval. Also corresponding to Conneau et al. the BiLSTM max pooling model performs the best, both for NLI and for SentEval. Interestingly however, the base model outperforms the LSTM for SentEval, and performs equal to BiLSTM.\n",
    "\n",
    "Performance scores for the NLI tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c5b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MR</th>\n",
       "      <th>CR</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>MPQA</th>\n",
       "      <th>SST2</th>\n",
       "      <th>TREC</th>\n",
       "      <th>SICKEntailment</th>\n",
       "      <th>MRPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>74.88</td>\n",
       "      <td>78.46</td>\n",
       "      <td>90.11</td>\n",
       "      <td>84.85</td>\n",
       "      <td>78.21</td>\n",
       "      <td>67.31</td>\n",
       "      <td>80.8</td>\n",
       "      <td>71.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>72.13</td>\n",
       "      <td>77.73</td>\n",
       "      <td>85.96</td>\n",
       "      <td>84.84</td>\n",
       "      <td>76.26</td>\n",
       "      <td>61.45</td>\n",
       "      <td>82.2</td>\n",
       "      <td>73.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstm</th>\n",
       "      <td>72.40</td>\n",
       "      <td>79.26</td>\n",
       "      <td>89.52</td>\n",
       "      <td>85.06</td>\n",
       "      <td>78.67</td>\n",
       "      <td>73.06</td>\n",
       "      <td>83.4</td>\n",
       "      <td>72.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstmpool</th>\n",
       "      <td>75.76</td>\n",
       "      <td>81.78</td>\n",
       "      <td>91.57</td>\n",
       "      <td>85.64</td>\n",
       "      <td>79.93</td>\n",
       "      <td>75.26</td>\n",
       "      <td>84.4</td>\n",
       "      <td>74.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MR     CR   SUBJ   MPQA   SST2   TREC  SICKEntailment   MRPC\n",
       "base        74.88  78.46  90.11  84.85  78.21  67.31            80.8  71.20\n",
       "lstm        72.13  77.73  85.96  84.84  76.26  61.45            82.2  73.11\n",
       "bilstm      72.40  79.26  89.52  85.06  78.67  73.06            83.4  72.55\n",
       "bilstmpool  75.76  81.78  91.57  85.64  79.93  75.26            84.4  74.31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('eval_results/final_task_results.json') as json_file:\n",
    "        sent_eval_scores = json.load(json_file)\n",
    "        \n",
    "sent_eval_results = pd.DataFrame.from_dict(sent_eval_scores, orient='index')\n",
    "sent_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5735130",
   "metadata": {},
   "source": [
    "The base model performs particularly well on SUBJ and MPQA. LSTM performs very low on TREC. BiLSTMPool performs the best on all tasks. The hardest task appears to be TREC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce8bcc",
   "metadata": {},
   "source": [
    "## Error analysis for NLI\n",
    "\n",
    "In this Section, I will try out various difficult NLI testcases. I will create 100 example sentences per testcase, and report on the performance of each model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90b0e7",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd742b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load bilstm encoder\n",
      "load bilstm maxpool encoder\n"
     ]
    }
   ],
   "source": [
    "base_nli, _ = mutils.load_model('base', 'model_checkpoints/base_model_final', None)\n",
    "lstm_nli, lstm_lstm = mutils.load_model('lstm', 'model_checkpoints/lstm_nli_model_final', 'model_checkpoints/lstm_lstm_model_final')\n",
    "bilstm_nli, bilstm_lstm = mutils.load_model('bilstm', 'model_checkpoints/bilstm_nli_model_final', 'model_checkpoints/bilstm_lstm_model_final')\n",
    "bilstmpool_nli, bilstmpool_lstm = mutils.load_model('bilstmpool', 'model_checkpoints/bilstmpool_nli_model_final', 'model_checkpoints/bilstmpool_lstm_model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66d5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_word_dict.json') as json_file:\n",
    "    word_dict = json.load(json_file)\n",
    "\n",
    "embedding_model = mutils.load_embeddings(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c20cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 0, 1:0, 2:0}\n",
    "\n",
    "results_dict = {'base': copy.deepcopy(label_dict),\n",
    "               'lstm' : copy.deepcopy(label_dict),\n",
    "               'bilstm': copy.deepcopy(label_dict),\n",
    "               'bilstmpool': copy.deepcopy(label_dict)}\n",
    "\n",
    "model_tuples = [('base', base_nli),('lstm', lstm_nli), ('bilstm', bilstm_nli), ('bilstmpool', bilstmpool_nli)]\n",
    "\n",
    "def predict_all_models(result_dict, s1, s2):\n",
    "    for model, nli in model_tuples:\n",
    "        prediction = mutils.predict(model, embedding_model, nli, s1, s2)\n",
    "        result_dict[model][prediction] += 1\n",
    "    return result_dict\n",
    "\n",
    "def print_performances(result_dict, nsamples, correct_label):\n",
    "    performances = {}\n",
    "    for model, _ in model_tuples:\n",
    "        model_scores = copy.deepcopy(result_dict[model])\n",
    "        model_scores['correct'] = str(round(model_scores[correct_label] / nsamples * 100,1)) + '%'\n",
    "        performances[model] = model_scores\n",
    "    performance_table = pd.DataFrame.from_dict(performances, orient='index')\n",
    "    print(performance_table)\n",
    "\n",
    "def eval_test(eval_sents, nsamples, correct_label):\n",
    "    result_dict = copy.deepcopy(results_dict)\n",
    "    for n, sent in enumerate(eval_sents):\n",
    "        try:\n",
    "            s1, s2 = sent.split(';')\n",
    "            \n",
    "        except:\n",
    "            nsamples = nsamples - 1\n",
    "            continue\n",
    "        if n == 0:\n",
    "            print(f\"{nsamples} samples of the following structure where the correct label is {correct_label}:\")\n",
    "            print(f\"Premise: '{s1}' \\nHypothesis '{s2}'\")\n",
    "        result_dict = predict_all_models(result_dict, s1, s2)\n",
    "    print_performances(result_dict, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116db77",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f34bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a6f8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "negation = editor.template('{first_name} is from {country}; {first_name} is not from {country} ')\n",
    "\n",
    "np.random.seed(seed) \n",
    "negation_sents = np.random.choice(negation.data, nsamples)\n",
    "correct_label = 2 #contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c13a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 2:\n",
      "Premise: 'Scott is from Poland' \n",
      "Hypothesis ' Scott is not from Poland '\n",
      "             0   1   2 correct\n",
      "base        43  56   1    1.0%\n",
      "lstm         1   0  99   99.0%\n",
      "bilstm       1   0  99   99.0%\n",
      "bilstmpool   3   0  97   97.0%\n"
     ]
    }
   ],
   "source": [
    "eval_test(negation_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f54639",
   "metadata": {},
   "source": [
    "## Negetion in one part of the sentence, entailment in the relevant part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5434d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0:\n",
      "Premise: 'Dave does not live in Milwaukee but they are from France' \n",
      "Hypothesis ' Dave is from France '\n",
      "             0   1   2 correct\n",
      "base         2  66  32    2.0%\n",
      "lstm        88   0  12   88.0%\n",
      "bilstm      18   7  75   18.0%\n",
      "bilstmpool  88   0  12   88.0%\n"
     ]
    }
   ],
   "source": [
    "negation_entailment = editor.template('{first_name} does not live in {city} but they are from {country}; {first_name} is from {country} ')\n",
    "negation_entailment_sents = np.random.choice(negation_entailment.data, nsamples)\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(negation_entailment_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4911b09",
   "metadata": {},
   "source": [
    "### TODO: interpret..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e4a45",
   "metadata": {},
   "source": [
    "## Active - passive / Subject - object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b571f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0:\n",
      "Premise: 'Marilyn silenced Joseph' \n",
      "Hypothesis ' Joseph was silenced by Marilyn'\n",
      "              0   1  2 correct\n",
      "base         97   3  0   97.0%\n",
      "lstm        100   0  0  100.0%\n",
      "bilstm      100   0  0  100.0%\n",
      "bilstmpool   69  30  1   69.0%\n"
     ]
    }
   ],
   "source": [
    "# a list of verbs to use in the test cases\n",
    "passive_verbs = ['kissed', 'killed', 'hurt', 'touched', 'ignored', 'silenced', 'hit', 'greeted']\n",
    "english_firstname = editor.lexicons.female_from.United_Kingdom + editor.lexicons.male_from.United_Kingdom\n",
    "\n",
    "active_passive = editor.template('{first_name} {verb} {first}; {first} was {verb} by {first_name}', first=english_firstname, verb=passive_verbs)\n",
    "active_passive_sents = np.random.choice(active_passive.data, nsamples)\n",
    "\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(active_passive_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "411939d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 1:\n",
      "Premise: 'Alex greeted Louisa' \n",
      "Hypothesis ' Alex was greeted by Louisa'\n",
      "              0  1  2 correct\n",
      "base         99  1  0    1.0%\n",
      "lstm        100  0  0    0.0%\n",
      "bilstm      100  0  0    0.0%\n",
      "bilstmpool  100  0  0    0.0%\n"
     ]
    }
   ],
   "source": [
    "# a list of verbs to use in the test cases\n",
    "passive_verbs = ['kissed', 'killed', 'hurt', 'touched', 'ignored', 'silenced', 'hit', 'greeted']\n",
    "english_firstname = editor.lexicons.female_from.United_Kingdom + editor.lexicons.male_from.United_Kingdom\n",
    "\n",
    "\n",
    "active_passive = editor.template('{first_name} {verb} {first}; {first_name} was {verb} by {first}', first=english_firstname, verb=passive_verbs)\n",
    "active_passive_sents = np.random.choice(active_passive.data, nsamples)\n",
    "\n",
    "correct_label = 1 #neutral\n",
    "\n",
    "eval_test(active_passive_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd8417",
   "metadata": {},
   "source": [
    "## Short vs long distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e7b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "verbs = ['hit', 'kicked', 'stopped', 'touched', 'missed', 'smashed']\n",
    "# lists of sentence fillers to increase the distance between the agent and the predicate\n",
    "# for active sentences\n",
    "precedents = ['nearly falling down', 'missing the past three games', 'celebrating a perfect streak', 'suffering from a knee injury', 'appearing so fit']\n",
    "# for passive sentences\n",
    "ball_precedents = ['lying there for a while', 'a boring match', 'three nerve-wrecking minutes', 'some time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef49e243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0:\n",
      "Premise: 'Ron, after celebrating a perfect streak, finnaly kicked the ball' \n",
      "Hypothesis ' Ron kicked the ball'\n",
      "              0   1   2 correct\n",
      "base         16  37  47   16.0%\n",
      "lstm         98   0   2   98.0%\n",
      "bilstm       97   3   0   97.0%\n",
      "bilstmpool  100   0   0  100.0%\n"
     ]
    }
   ],
   "source": [
    "long_distance = editor.template(\"{first_name}, after {filler}, finnaly {verb} the ball; {first_name} {verb} the ball\", verb=verbs, filler=precedents)\n",
    "long_d_sents = np.random.choice(long_distance.data, nsamples)\n",
    "\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(long_d_sents, nsamples, correct_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0567f0",
   "metadata": {},
   "source": [
    "## Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9679b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of the following structure where the correct label is 0:\n",
      "Premise: 'Kate, is a hairdresser' \n",
      "Hypothesis ' Kate a hairstylist'\n",
      "             0   1   2 correct\n",
      "base         7  85   8    7.0%\n",
      "lstm        48  29  23   48.0%\n",
      "bilstm      67  23  10   67.0%\n",
      "bilstmpool  83  15   2   83.0%\n"
     ]
    }
   ],
   "source": [
    "synonym_words = [('author', 'writer'), ('surgeon', 'doctor'), ('server', 'waiter'), ('chef','cook'), \n",
    "                 ('educator','teacher'), ('professor','academic'), ('person','human'), ('actor','performer'),\n",
    "                 ('musician', 'artist'), ('hairdresser', 'hairstylist')]\n",
    "\n",
    "synonyms = editor.template(\"{first_name}, is {a:occupation[0]}; {first_name} {a:occupation[1]}\", occupation=synonym_words, filler=precedents)\n",
    "synonym_sents = np.random.choice(synonyms.data, nsamples)\n",
    "\n",
    "correct_label = 0 #entailment\n",
    "\n",
    "eval_test(synonym_sents, nsamples, correct_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1e1b1",
   "metadata": {},
   "source": [
    "# Sent embeddings evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58010f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
